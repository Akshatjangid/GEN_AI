{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b72da4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "44d7d81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e2c40ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f58b9efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage,SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a1894738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000293AF41E620>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000293AF41E770>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=init_chat_model(\"groq:llama-3.1-8b-instant\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2c314572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000293AF7C31C0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000293AF7C17B0>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm=ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "89e2912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create message\n",
    "messages=[\n",
    "    SystemMessage(\"you are a helpful AI assistant\"),\n",
    "    HumanMessage(\"What are the top 3 benifits of langchain?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02c502e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Langchain is a platform that enables users to build and deploy AI models, particularly those that are multimodal (able to process and understand text, images, and other forms of data). Here are three potential benefits of Langchain:\\n\\n1. **Multimodal AI capabilities**: Langchain allows developers to create AI models that can process and understand multiple forms of data, such as text, images, and audio. This enables a wide range of applications, including image classification, object detection, natural language processing, and more. The ability to integrate multiple data types into a single model can lead to more accurate and informative results.\\n\\n2. **Improved AI Model Customization**: Langchain enables developers to fine-tune pre-trained AI models to specific use cases and domains, which can lead to more accurate and relevant results. This is achieved through a process called \"prompt engineering,\" where developers can craft custom prompts to elicit specific responses from the AI model. By tailoring the model to a specific domain or use case, developers can achieve better performance and more relevant results.\\n\\n3. **Efficient and Scalable AI Development**: Langchain provides a range of tools and APIs that make it easier and faster to develop and deploy AI models. The platform includes a range of features, such as automated model training, model deployment, and model serving, which can help developers to streamline their development process and reduce the time and effort required to bring an AI project to production.\\n\\nThese benefits can lead to a wide range of applications and use cases, including but not limited to:\\n\\n- Improved customer service through chatbots and virtual assistants\\n- Enhanced image and video analysis for applications such as healthcare and surveillance\\n- More accurate and informative natural language processing for applications such as sentiment analysis and language translation\\n- More efficient and scalable AI development for applications such as predictive maintenance and demand forecasting', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 370, 'prompt_tokens': 54, 'total_tokens': 424, 'completion_time': 0.609896316, 'prompt_time': 0.057866467, 'queue_time': 0.046038783, 'total_time': 0.667762783}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c40956ddc4', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--205b8857-3506-498d-b79f-a322b0f4eddd-0', usage_metadata={'input_tokens': 54, 'output_tokens': 370, 'total_tokens': 424})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ## invoke the model\n",
    "response=model.invoke(messages)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e416ef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is a platform that enables users to build and deploy AI models, particularly those that are multimodal (able to process and understand text, images, and other forms of data). Here are three potential benefits of Langchain:\n",
      "\n",
      "1. **Multimodal AI capabilities**: Langchain allows developers to create AI models that can process and understand multiple forms of data, such as text, images, and audio. This enables a wide range of applications, including image classification, object detection, natural language processing, and more. The ability to integrate multiple data types into a single model can lead to more accurate and informative results.\n",
      "\n",
      "2. **Improved AI Model Customization**: Langchain enables developers to fine-tune pre-trained AI models to specific use cases and domains, which can lead to more accurate and relevant results. This is achieved through a process called \"prompt engineering,\" where developers can craft custom prompts to elicit specific responses from the AI model. By tailoring the model to a specific domain or use case, developers can achieve better performance and more relevant results.\n",
      "\n",
      "3. **Efficient and Scalable AI Development**: Langchain provides a range of tools and APIs that make it easier and faster to develop and deploy AI models. The platform includes a range of features, such as automated model training, model deployment, and model serving, which can help developers to streamline their development process and reduce the time and effort required to bring an AI project to production.\n",
      "\n",
      "These benefits can lead to a wide range of applications and use cases, including but not limited to:\n",
      "\n",
      "- Improved customer service through chatbots and virtual assistants\n",
      "- Enhanced image and video analysis for applications such as healthcare and surveillance\n",
      "- More accurate and informative natural language processing for applications such as sentiment analysis and language translation\n",
      "- More efficient and scalable AI development for applications such as predictive maintenance and demand forecasting\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "13083ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Machine learning is a subset of artificial intelligence (AI) that involves the use of algorithms and statistical models to enable machines to learn from data, make decisions, and improve their performance over time without being explicitly programmed.\\n\\nMachine learning works by feeding a machine learning model with a large dataset, and the model learns patterns, relationships, and trends within the data. This process allows the model to make predictions, classify objects, and make decisions based on the data it has learned.\\n\\nThere are several types of machine learning:\\n\\n1. **Supervised learning**: The model is trained on labeled data, where the correct output is already known. The model learns to make predictions based on the input data.\\n2. **Unsupervised learning**: The model is trained on unlabeled data, and it must find patterns or relationships within the data on its own.\\n3. **Reinforcement learning**: The model learns by interacting with an environment and receiving feedback in the form of rewards or penalties.\\n4. **Semi-supervised learning**: The model is trained on a combination of labeled and unlabeled data.\\n\\nMachine learning has many applications, including:\\n\\n1. **Image and speech recognition**: Machine learning models can be trained to recognize objects, faces, and speech patterns.\\n2. **Natural language processing**: Machine learning models can be trained to understand and generate human language.\\n3. **Predictive analytics**: Machine learning models can be trained to predict outcomes based on historical data.\\n4. **Recommendation systems**: Machine learning models can be trained to recommend products or services based on user behavior.\\n5. **Autonomous vehicles**: Machine learning models can be trained to control self-driving cars.\\n\\nMachine learning models can be trained using various algorithms, including:\\n\\n1. **Linear regression**: A linear model that predicts a continuous output variable.\\n2. **Decision trees**: A model that makes decisions based on a tree-like structure.\\n3. **Random forests**: An ensemble model that combines multiple decision trees.\\n4. **Support vector machines**: A model that finds the optimal hyperplane to separate classes.\\n5. **Neural networks**: A model inspired by the structure and function of the human brain.\\n6. **Gradient boosting**: An ensemble model that combines multiple weak models to create a strong model.\\n\\nThe benefits of machine learning include:\\n\\n1. **Improved accuracy**: Machine learning models can make more accurate predictions and decisions than traditional models.\\n2. **Increased efficiency**: Machine learning models can automate many tasks, reducing the need for human intervention.\\n3. **Scalability**: Machine learning models can handle large datasets and scale to meet the needs of complex applications.\\n4. **Flexibility**: Machine learning models can be trained on a wide range of data types and applications.\\n\\nHowever, machine learning also has some challenges and limitations, including:\\n\\n1. **Data quality**: Machine learning models require high-quality data to learn effectively.\\n2. **Bias and fairness**: Machine learning models can perpetuate biases and unfairness if the data is biased.\\n3. **Interpretability**: Machine learning models can be difficult to interpret, making it challenging to understand how they make decisions.\\n4. **Overfitting**: Machine learning models can overfit the training data, resulting in poor performance on new data.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 651, 'prompt_tokens': 39, 'total_tokens': 690, 'completion_time': 0.857050871, 'prompt_time': 0.12858132, 'queue_time': 0.059835259, 'total_time': 0.985632191}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_c40956ddc4', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--a8b5c1ef-8305-411b-bd64-14af8cef3cc8-0', usage_metadata={'input_tokens': 39, 'output_tokens': 651, 'total_tokens': 690})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(\"What is machine learning\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b4b3fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework for building large language models (LLMs) and their applications. Here are three of the top benefits of using LangChain:\n",
      "\n",
      "1. **Easy Integration with LLMs**: LangChain provides a simple and seamless way to integrate with popular LLMs like LLaMA, Meta AI's Llama, and more. This makes it easier to build applications that leverage the power of LLMs, without requiring extensive knowledge of the underlying model architecture.\n",
      "\n",
      "2. **Modular Architecture**: LangChain's modular design allows developers to break down complex tasks into smaller, manageable components. This makes it easier to build and maintain applications, as each component can be modified or replaced independently without affecting the rest of the system.\n",
      "\n",
      "3. **Extensive Range of Tools and Functions**: LangChain includes a wide range of tools and functions for building and deploying LLM-based applications. These include support for tasks like data loading, model training, and deployment, as well as functions for natural language processing, text generation, and more. This makes it easier to build applications that can perform a variety of tasks, from simple text classification to complex text generation.\n",
      "\n",
      "These benefits make LangChain a powerful tool for developers and researchers looking to build and deploy LLM-based applications."
     ]
    }
   ],
   "source": [
    "# streamin exemple\n",
    "for chunk in model.stream(messages):\n",
    "    print(chunk.content,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f6ce26",
   "metadata": {},
   "source": [
    "# Dynamic Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8868ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "## create translation app\n",
    "\n",
    "translation_template=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"you are a professional translator.Translate the following {text} from {source_language} to {target_language}.Maintain the tone and style\"),\n",
    "    (\"user\",\"{text}\")\n",
    "])\n",
    "\n",
    "## using template\n",
    "prompt=translation_template.invoke({\n",
    "    \"source_language\":\"English\",\n",
    "    \"target_language\":\"spanish\",\n",
    "    \"text\":\"Langchain makes building AI application incredibility easy!\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "35789e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='you are a professional translator.Translate the following Langchain makes building AI application incredibility easy! from English to spanish.Maintain the tone and style', additional_kwargs={}, response_metadata={}), HumanMessage(content='Langchain makes building AI application incredibility easy!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8177c923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain hace que construir aplicaciones de IA sea increíblemente fácil.\n"
     ]
    }
   ],
   "source": [
    "translated_response=model.invoke(prompt)\n",
    "print(translated_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cb593f",
   "metadata": {},
   "source": [
    "# building your first chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "edc127b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough ,RunnableLambda\n",
    "\n",
    "\n",
    "def create_story_chain():\n",
    "    ## template for story genration\n",
    "    story_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"you are a creative storyteller. Write a short and engaging story based on a given theme, character, and setting\"),\n",
    "            (\"user\", \"Theme: {theme}\\nMain character: {character}\\nSetting: {setting}\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # template for story anylysis\n",
    "\n",
    "    anylysis_prompt=ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"you are a litrary critic. Anylyze the following story and provide insights.\"),\n",
    "        (\"user\",\"{story}\")\n",
    "    ])\n",
    "\n",
    "    # create a story function to pass a story to anylysis\n",
    "    def analyze_story(story_text):\n",
    "        return {\"story\":story_text}\n",
    "    story_chain=(\n",
    "        story_prompt| model | StrOutputParser\n",
    "    )\n",
    "    anylysis_chain=(\n",
    "        story_chain\n",
    "        | RunnableLambda(analyze_story)\n",
    "        | anylysis_prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return anylysis_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4d936a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are a creative storyteller. Write a short and engaging story based on a given theme, character, and setting'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, template='Theme: {theme}\\nMain character: {character}\\nSetting: {setting}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000293AF41E620>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000293AF41E770>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| RunnableLambda(StrOutputParser)\n",
       "| RunnableLambda(analyze_story)\n",
       "| ChatPromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are a litrary critic. Anylyze the following story and provide insights.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, template='{story}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000293AF41E620>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000293AF41E770>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=create_story_chain()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "684c573b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are a creative storyteller. Write a short and engaging story based on a given theme, character, and setting'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, template='Theme: {theme}\\nMain character: {character}\\nSetting: {setting}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000293AF41E620>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000293AF41E770>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| RunnableLambda(StrOutputParser)\n",
       "| RunnableLambda(analyze_story)\n",
       "| ChatPromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='you are a litrary critic. Anylyze the following story and provide insights.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, template='{story}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x00000293AF41E620>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x00000293AF41E770>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain=create_story_chain()\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6e3d2704",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtheme\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43martificial intelligence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcharacter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma curious robot\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msetting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma futuristic city\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStory and Anylysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\AKSHAT_CODE\\GEN_AI\\Krish_Nail_LLM\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:3082\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, input_, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3081\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3082\u001b[0m                 input_ \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3083\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\AKSHAT_CODE\\GEN_AI\\Krish_Nail_LLM\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4828\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4813\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this ``Runnable`` synchronously.\u001b[39;00m\n\u001b[0;32m   4814\u001b[0m \n\u001b[0;32m   4815\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4825\u001b[0m \n\u001b[0;32m   4826\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 4828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m   4829\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invoke,\n\u001b[0;32m   4830\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4831\u001b[0m         ensure_config(config),\n\u001b[0;32m   4832\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4833\u001b[0m     )\n\u001b[0;32m   4834\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4835\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\AKSHAT_CODE\\GEN_AI\\Krish_Nail_LLM\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:1953\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[0;32m   1949\u001b[0m     child_config \u001b[38;5;241m=\u001b[39m patch_config(config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child())\n\u001b[0;32m   1950\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[0;32m   1951\u001b[0m         output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m-> 1953\u001b[0m             context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1954\u001b[0m                 call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1955\u001b[0m                 func,\n\u001b[0;32m   1956\u001b[0m                 input_,\n\u001b[0;32m   1957\u001b[0m                 config,\n\u001b[0;32m   1958\u001b[0m                 run_manager,\n\u001b[0;32m   1959\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1960\u001b[0m             ),\n\u001b[0;32m   1961\u001b[0m         )\n\u001b[0;32m   1962\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1963\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\AKSHAT_CODE\\GEN_AI\\Krish_Nail_LLM\\.venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    428\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\AKSHAT_CODE\\GEN_AI\\Krish_Nail_LLM\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4685\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[1;34m(self, input_, run_manager, config, **kwargs)\u001b[0m\n\u001b[0;32m   4683\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[0;32m   4684\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4685\u001b[0m     output \u001b[38;5;241m=\u001b[39m call_func_with_variable_args(\n\u001b[0;32m   4686\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, input_, config, run_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   4687\u001b[0m     )\n\u001b[0;32m   4688\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[0;32m   4689\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\AKSHAT_CODE\\GEN_AI\\Krish_Nail_LLM\\.venv\\lib\\site-packages\\langchain_core\\runnables\\config.py:429\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    428\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Akshat\\AKSHAT_CODE\\GEN_AI\\Krish_Nail_LLM\\.venv\\lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: BaseModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\n",
    "    \"theme\": \"artificial intelligence\",\n",
    "    \"character\": \"a curious robot\",\n",
    "    \"setting\": \"a futuristic city\"\n",
    "})\n",
    "\n",
    "\n",
    "print(\"Story and Anylysis:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d251f79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "krish-nail-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
